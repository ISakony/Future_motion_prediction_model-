{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 予測動作フレーム生成処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) 必要モデル準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import chainer\n",
    "from chainer import cuda, serializers, functions as F\n",
    "from entity import params, JointType\n",
    "from models.CocoPoseNet import CocoPoseNet\n",
    "import chainer.links as L\n",
    "from chainer import Chain, Variable, cuda, optimizer, optimizers, serializers\n",
    "import pickle\n",
    "import os\n",
    "from chainer.links import caffe\n",
    "\n",
    "\n",
    "class PoseDetector(object):\n",
    "    def __init__(self, arch=None, weights_file=None, model=None, device=-1):\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "        else:\n",
    "            # load model\n",
    "            print('Loading PoseNet...')\n",
    "            self.model = params['archs'][arch]()\n",
    "            if weights_file:\n",
    "                serializers.load_npz(weights_file, self.model)\n",
    "\n",
    "        self.device = device\n",
    "        if self.device >= 0:\n",
    "            cuda.get_device_from_id(device).use()\n",
    "            self.model.to_gpu()\n",
    "\n",
    "            # create gaussian filter\n",
    "            ksize = params['ksize']\n",
    "            kernel = cuda.to_gpu(self.create_gaussian_kernel(sigma=params['gaussian_sigma'], ksize=ksize))\n",
    "            self.gaussian_kernel = kernel\n",
    "\n",
    "    # compute gaussian filter\n",
    "    def create_gaussian_kernel(self, sigma=1, ksize=5):\n",
    "        center = int(ksize / 2)\n",
    "        kernel = np.zeros((1, 1, ksize, ksize), dtype=np.float32)\n",
    "        for y in range(ksize):\n",
    "            distance_y = abs(y-center)\n",
    "            for x in range(ksize):\n",
    "                distance_x = abs(x-center)\n",
    "                kernel[0][0][y][x] = 1/(sigma**2 * 2 * np.pi) * np.exp(-(distance_x**2 + distance_y**2)/(2 * sigma**2))\n",
    "        return kernel\n",
    "\n",
    "    def compute_optimal_size(self, orig_img, img_size):\n",
    "        \"\"\"画像のサイズが幅と高さが8の倍数になるように調節する\"\"\"\n",
    "        orig_img_h, orig_img_w, _ = orig_img.shape\n",
    "        aspect = orig_img_h / orig_img_w\n",
    "        if orig_img_h < orig_img_w:\n",
    "            img_h = img_size\n",
    "            img_w = np.round(img_size / aspect).astype(int)\n",
    "            surplus = img_w % 8\n",
    "            if surplus != 0:\n",
    "                img_w += 8 - surplus\n",
    "        else:\n",
    "            img_w = img_size\n",
    "            img_h = np.round(img_size * aspect).astype(int)\n",
    "            surplus = img_h % 8\n",
    "            if surplus != 0:\n",
    "                img_h += 8 - surplus\n",
    "        return (img_w, img_h)\n",
    "\n",
    "    def compute_peaks_from_heatmaps(self, heatmaps):\n",
    "        heatmaps = heatmaps[:-1]\n",
    "\n",
    "        xp = cuda.get_array_module(heatmaps)\n",
    "\n",
    "        if xp == np:\n",
    "            all_peaks = []\n",
    "            peak_counter = 0\n",
    "            for i , heatmap in enumerate(heatmaps):\n",
    "                heatmap = gaussian_filter(heatmap, sigma=params['gaussian_sigma'])\n",
    "                map_left = xp.zeros(heatmap.shape)\n",
    "                map_right = xp.zeros(heatmap.shape)\n",
    "                map_top = xp.zeros(heatmap.shape)\n",
    "                map_bottom = xp.zeros(heatmap.shape)\n",
    "                map_left[1:, :] = heatmap[:-1, :]\n",
    "                map_right[:-1, :] = heatmap[1:, :]\n",
    "                map_top[:, 1:] = heatmap[:, :-1]\n",
    "                map_bottom[:, :-1] = heatmap[:, 1:]\n",
    "\n",
    "                peaks_binary = xp.logical_and.reduce((heatmap >= map_left, heatmap >= map_right, heatmap >= map_top, heatmap >= map_bottom, heatmap > params['heatmap_peak_thresh']))\n",
    "                peaks = zip(xp.nonzero(peaks_binary)[1], xp.nonzero(peaks_binary)[0]) # [(x, y), (x, y)...]のpeak座標配列\n",
    "                peaks_with_score = [(i,) + peak_pos + (heatmap[peak_pos[1], peak_pos[0]],) for peak_pos in peaks] # [(x, y, score), (x, y, score)...]のpeak配列 scoreはheatmap上のscore\n",
    "                peaks_id = range(peak_counter, peak_counter + len(peaks_with_score))\n",
    "                peaks_with_score_and_id = [peaks_with_score[i] + (peaks_id[i], ) for i in range(len(peaks_id))] # [(x, y, score, id), (x, y, score, id)...]のpeak配列\n",
    "                peak_counter += len(peaks_with_score_and_id)\n",
    "                all_peaks.append(peaks_with_score_and_id)\n",
    "            all_peaks = np.array([peak for peaks_each_category in all_peaks for peak in peaks_each_category])\n",
    "        else:\n",
    "            heatmaps = F.convolution_2d(heatmaps[:, None], self.gaussian_kernel, stride=1, pad=int(params['ksize']/2)).data.squeeze()\n",
    "            left_heatmaps = xp.zeros(heatmaps.shape)\n",
    "            right_heatmaps = xp.zeros(heatmaps.shape)\n",
    "            top_heatmaps = xp.zeros(heatmaps.shape)\n",
    "            bottom_heatmaps = xp.zeros(heatmaps.shape)\n",
    "            left_heatmaps[:, 1:, :] = heatmaps[:, :-1, :]\n",
    "            right_heatmaps[:, :-1, :] = heatmaps[:, 1:, :]\n",
    "            top_heatmaps[:, :, 1:] = heatmaps[:, :, :-1]\n",
    "            bottom_heatmaps[:, :, :-1] = heatmaps[:, :, 1:]\n",
    "\n",
    "            peaks_binary = xp.logical_and(heatmaps >= left_heatmaps, heatmaps >= right_heatmaps)\n",
    "            peaks_binary = xp.logical_and(peaks_binary, heatmaps >= top_heatmaps)\n",
    "            peaks_binary = xp.logical_and(peaks_binary, heatmaps >= bottom_heatmaps)\n",
    "            peaks_binary = xp.logical_and(peaks_binary, heatmaps >= params['heatmap_peak_thresh'])\n",
    "\n",
    "            peak_c, peak_y, peak_x = xp.nonzero(peaks_binary)\n",
    "            peak_score = heatmaps[peak_c, peak_y, peak_x]\n",
    "            all_peaks = xp.vstack((peak_c, peak_x, peak_y, peak_score)).transpose()\n",
    "            all_peaks = xp.hstack((all_peaks, xp.arange(len(all_peaks)).reshape(-1, 1)))\n",
    "            all_peaks = all_peaks.get()\n",
    "        return all_peaks\n",
    "\n",
    "    def extract_paf_in_points(self, paf, points):\n",
    "        paf_in_edge = []\n",
    "\n",
    "        for point in points:\n",
    "            point_x = int(round(point[0]))\n",
    "            point_y = int(round(point[1]))\n",
    "            paf_in_edge.append([paf[0, point_y, point_x], paf[1, point_y, point_x]])\n",
    "\n",
    "        return paf_in_edge\n",
    "\n",
    "    def compute_candidate_connections_greedy(self, paf, cand_a, cand_b, img_len, params):\n",
    "        candidate_connections = []\n",
    "\n",
    "        for index_a, joint_a in enumerate(cand_a):\n",
    "            for index_b, joint_b in enumerate(cand_b): # jointは(x, y)座標\n",
    "                vec = np.subtract(joint_b[:2], joint_a[:2])\n",
    "                vec_len = np.linalg.norm(vec)\n",
    "                if vec_len == 0:\n",
    "                    continue\n",
    "\n",
    "                vec_unit = vec / vec_len\n",
    "                integ_points = zip(\n",
    "                    np.linspace(joint_a[0], joint_b[0], num=params['n_integ_points']),\n",
    "                    np.linspace(joint_a[1], joint_b[1], num=params['n_integ_points'])\n",
    "                ) # joint_aとjoint_bの2点間を結ぶ線分上の座標点 [[x1, y1], [x2, y2]...]\n",
    "\n",
    "                paf_in_edge = self.extract_paf_in_points(paf, integ_points)\n",
    "                inner_products = np.dot(paf_in_edge, vec_unit)\n",
    "\n",
    "                integ_value = np.sum(inner_products) / len(inner_products)\n",
    "                integ_value_with_dist_prior = integ_value + min(params['length_penalty_ratio'] * img_len / vec_len - 1, 0) # vectorの長さが1以上の時にペナルティを与える(0 ~ 0.75、長いほどペナルティが大きい)\n",
    "\n",
    "                n_valid_points = len(np.nonzero(inner_products > params['inner_product_thresh'])[0])\n",
    "                if n_valid_points > params['n_integ_points_thresh'] and integ_value_with_dist_prior > 0:\n",
    "                    candidate_connections.append([index_a, index_b, integ_value_with_dist_prior, integ_value_with_dist_prior + joint_a[2] + joint_b[2]])\n",
    "\n",
    "        candidate_connections = sorted(candidate_connections, key=lambda x: x[2], reverse=True)\n",
    "        return candidate_connections\n",
    "\n",
    "    def compute_connections(self, pafs, all_peaks, img_len, params):\n",
    "        all_connections = []\n",
    "        for i in range(len(params['limbs_point'])):\n",
    "            paf_index = [i * 2, i * 2 + 1]\n",
    "            paf = pafs[paf_index]\n",
    "            limb_point = params['limbs_point'][i]\n",
    "            cand_a = all_peaks[all_peaks[:, 0] == limb_point[0]][:, 1:]\n",
    "            cand_b = all_peaks[all_peaks[:, 0] == limb_point[1]][:, 1:]\n",
    "\n",
    "            if len(cand_a) > 0 and len(cand_b) > 0:\n",
    "                candidate_connections = self.compute_candidate_connections_greedy(paf, cand_a, cand_b, img_len, params)\n",
    "                connections = np.zeros((0, 5))\n",
    "                for c in candidate_connections:\n",
    "                    index_a, index_b, score = c[0:3]\n",
    "                    if index_a not in connections[:, 3] and index_b not in connections[:, 4]:\n",
    "                        connections = np.vstack([connections, [cand_a[index_a][3], cand_b[index_b][3], score, index_a, index_b]])\n",
    "                        if len(connections) >= min(len(cand_a), len(cand_b)):\n",
    "                            break\n",
    "\n",
    "                all_connections.append(connections)\n",
    "            else:\n",
    "                all_connections.append(np.zeros((0, 5)))\n",
    "        return all_connections\n",
    "\n",
    "    def grouping_key_points(self, all_connections, candidate_peaks, params):\n",
    "        subsets = -1 * np.ones((0, 20))\n",
    "\n",
    "        for connection_category_index in range(len(params['limbs_point'])):\n",
    "            paf_index = [connection_category_index * 2, connection_category_index * 2 + 1]\n",
    "            joint_a_indices = all_connections[connection_category_index][:, 0]\n",
    "            joint_b_indices = all_connections[connection_category_index][:, 1]\n",
    "            joint_category_a_index, joint_category_b_index = params['limbs_point'][connection_category_index] # カテゴリのindex\n",
    "\n",
    "            for connection_index, _ in enumerate(all_connections[connection_category_index]):\n",
    "                joint_found_cnt = 0\n",
    "                joint_found_subset_index = [-1, -1]\n",
    "                for subset_index, subset in enumerate(subsets):\n",
    "                    # そのconnectionのjointをもってるsubsetがいる場合\n",
    "                    if subset[joint_category_a_index] == joint_a_indices[connection_index] or subset[joint_category_b_index] == joint_b_indices[connection_index]:\n",
    "                        joint_found_subset_index[joint_found_cnt] = subset_index\n",
    "                        joint_found_cnt += 1\n",
    "\n",
    "                if joint_found_cnt == 1: # そのconnectionのどちらかのjointをsubsetが持っている場合\n",
    "                    found_subset = subsets[joint_found_subset_index[0]]\n",
    "                    # 肩->耳のconnectionの組合せを除いて、始点の一致しか起こり得ない。肩->耳の場合、終点が一致していた場合は、既に顔のbone検出済みなので処理不要。\n",
    "                    if(found_subset[joint_category_b_index] != joint_b_indices[connection_index]):\n",
    "                        found_subset[joint_category_b_index] = joint_b_indices[connection_index]\n",
    "                        found_subset[-1] += 1 # increment joint count\n",
    "                        # joint bのscoreとconnectionの積分値を加算\n",
    "                        found_subset[-2] += candidate_peaks[joint_b_indices[connection_index].astype(int), 3] + all_connections[connection_category_index][connection_index][2]\n",
    "\n",
    "                elif joint_found_cnt == 2: # subset1にjoint1が、subset2にjoint2がある場合(肩->耳のconnectionの組合せした起こり得ない)\n",
    "                    found_subset_1 = subsets[joint_found_subset_index[0]]\n",
    "                    found_subset_2 = subsets[joint_found_subset_index[1]]\n",
    "\n",
    "                    membership = ((found_subset_1 >= 0).astype(int) + (found_subset_2 >= 0).astype(int))[:-2]\n",
    "                    if len(np.nonzero(membership == 2)[0]) == 0: # merge two subsets when no duplication\n",
    "                        found_subset_1[:-2] += found_subset_2[:-2] + 1 # default is -1\n",
    "                        found_subset_1[-2:] += found_subset_2[-2:]\n",
    "                        found_subset_1[-2:] += all_connections[connection_category_index][connection_index][2] # connectionの積分値のみ加算(jointのscoreはmerge時に全て加算済み)\n",
    "                        subsets = np.delete(subsets, joint_found_subset_index[1], 0)\n",
    "                    else:\n",
    "                        pass\n",
    "                        # found_subset_1[joint_category_b_index] = joint_b_indices[connection_index]\n",
    "                        # found_subset_1[-1] += 1 # increment joint count\n",
    "                        # found_subset_1[-2] += candidate_peaks[joint_b_indices[connection_index].astype(int), 3] + all_connections[connection_category_index][connection_index][2]\n",
    "                        # joint bのscoreとconnectionの積分値を加算\n",
    "\n",
    "                elif joint_found_cnt == 0 and connection_category_index < 17: # 肩耳のconnectionは新規group対象外\n",
    "                    row = -1 * np.ones(20)\n",
    "                    row[joint_category_a_index] = joint_a_indices[connection_index]\n",
    "                    row[joint_category_b_index] = joint_b_indices[connection_index]\n",
    "                    row[-1] = 2\n",
    "                    row[-2] = sum(candidate_peaks[all_connections[connection_category_index][connection_index, :2].astype(int), 3]) + all_connections[connection_category_index][connection_index][2]\n",
    "                    subsets = np.vstack([subsets, row])\n",
    "\n",
    "        # delete low score subsets\n",
    "        keep = np.logical_and(subsets[:, -1] >= params['n_subset_limbs_thresh'], subsets[:, -2]/subsets[:, -1] >= params['subset_score_thresh'])\n",
    "        subsets = subsets[keep]\n",
    "        return subsets\n",
    "\n",
    "    def subsets_to_person_pose_array(self, subsets, all_peaks):\n",
    "        person_pose_array = []\n",
    "        for subset in subsets:\n",
    "            joints = []\n",
    "            for joint_index in subset[:18].astype('i'):\n",
    "                if joint_index >= 0:\n",
    "                    joint = all_peaks[joint_index][1:3].astype('i').tolist()\n",
    "                    joint.append(2)\n",
    "                    joints.append(joint)\n",
    "                else:\n",
    "                    joints.append([0, 0, 0])\n",
    "            person_pose_array.append(np.array(joints))\n",
    "        person_pose_array = np.array(person_pose_array)\n",
    "        return person_pose_array\n",
    "\n",
    "    def compute_limbs_length(self, joints):\n",
    "        limbs = []\n",
    "        limbs_len = np.zeros(len(params[\"limbs_point\"]))\n",
    "        for i, joint_indices in enumerate(params[\"limbs_point\"]):\n",
    "            if joints[joint_indices[0]] is not None and joints[joint_indices[1]] is not None:\n",
    "                limbs.append([joints[joint_indices[0]], joints[joint_indices[1]]])\n",
    "                limbs_len[i] = np.linalg.norm(joints[joint_indices[1]][:-1] - joints[joint_indices[0]][:-1])\n",
    "            else:\n",
    "                limbs.append(None)\n",
    "\n",
    "        return limbs_len, limbs\n",
    "\n",
    "    def compute_unit_length(self, limbs_len):\n",
    "        unit_length = 0\n",
    "        base_limbs_len = limbs_len[[14, 3, 0, 13, 9]] # (鼻首、首左腰、首右腰、肩左耳、肩右耳)の長さの比率(このどれかが存在すればこれを優先的に単位長さの計算する)\n",
    "        non_zero_limbs_len = base_limbs_len > 0\n",
    "        if len(np.nonzero(non_zero_limbs_len)[0]) > 0:\n",
    "            limbs_len_ratio = np.array([0.85, 2.2, 2.2, 0.85, 0.85]) \n",
    "            unit_length = np.sum(base_limbs_len[non_zero_limbs_len] / limbs_len_ratio[non_zero_limbs_len]) / len(np.nonzero(non_zero_limbs_len)[0])\n",
    "        else:\n",
    "            limbs_len_ratio = np.array([2.2, 1.7, 1.7, 2.2, 1.7, 1.7, 0.6, 0.93, 0.65, 0.85, 0.6, 0.93, 0.65, 0.85, 1, 0.2, 0.2, 0.25, 0.25])\n",
    "            non_zero_limbs_len = limbs_len > 0\n",
    "            unit_length = np.sum(limbs_len[non_zero_limbs_len] / limbs_len_ratio[non_zero_limbs_len]) / len(np.nonzero(non_zero_limbs_len)[0])\n",
    "\n",
    "        return unit_length\n",
    "\n",
    "    def get_unit_length(self, person_pose):\n",
    "        limbs_length, limbs = self.compute_limbs_length(person_pose)\n",
    "        unit_length = self.compute_unit_length(limbs_length)\n",
    "\n",
    "        return unit_length\n",
    "\n",
    "    def crop_around_keypoint(self, img, keypoint, crop_size):\n",
    "        x, y = keypoint\n",
    "        left = int(x - crop_size)\n",
    "        top = int(y - crop_size)\n",
    "        right = int(x + crop_size)\n",
    "        bottom = int(y + crop_size)\n",
    "        bbox = (left, top, right, bottom)\n",
    "\n",
    "        cropped_img = self.crop_image(img, bbox)\n",
    "\n",
    "        return cropped_img, bbox\n",
    "\n",
    "    def crop_person(self, img, person_pose, unit_length):\n",
    "        top_joint_priority = [4, 5, 6, 12, 16, 7, 13, 17, 8, 10, 14, 9, 11, 15, 2, 3, 0, 1, sys.maxsize]\n",
    "        bottom_joint_priority = [9, 6, 7, 14, 16, 8, 15, 17, 4, 2, 0, 5, 3, 1, 10, 11, 12, 13, sys.maxsize]\n",
    "\n",
    "        top_joint_index = len(top_joint_priority) - 1\n",
    "        bottom_joint_index = len(bottom_joint_priority) - 1\n",
    "        left_joint_index = 0\n",
    "        right_joint_index = 0\n",
    "        top_pos = sys.maxsize\n",
    "        bottom_pos = 0\n",
    "        left_pos = sys.maxsize\n",
    "        right_pos = 0\n",
    "\n",
    "        for i, joint in enumerate(person_pose):\n",
    "            if joint[2] > 0:\n",
    "                if top_joint_priority[i] < top_joint_priority[top_joint_index]:\n",
    "                    top_joint_index = i\n",
    "                elif bottom_joint_priority[i] < bottom_joint_priority[bottom_joint_index]:\n",
    "                    bottom_joint_index = i\n",
    "                if joint[1] < top_pos:\n",
    "                    top_pos = joint[1]\n",
    "                elif joint[1] > bottom_pos:\n",
    "                    bottom_pos = joint[1]\n",
    "\n",
    "                if joint[0] < left_pos:\n",
    "                    left_pos = joint[0]\n",
    "                    left_joint_index = i\n",
    "                elif joint[0] > right_pos:\n",
    "                    right_pos = joint[0]\n",
    "                    right_joint_index = i\n",
    "\n",
    "        top_padding_radio = [0.9, 1.9, 1.9, 2.9, 3.7, 1.9, 2.9, 3.7, 4.0, 5.5, 7.0, 4.0, 5.5, 7.0, 0.7, 0.8, 0.7, 0.8] \n",
    "        bottom_padding_radio = [6.9, 5.9, 5.9, 4.9, 4.1, 5.9, 4.9, 4.1, 3.8, 2.3, 0.8, 3.8, 2.3, 0.8, 7.1, 7.0, 7.1, 7.0]\n",
    "\n",
    "        left = (left_pos - 0.3 * unit_length).astype(int)\n",
    "        right = (right_pos + 0.3 * unit_length).astype(int)\n",
    "        top = (top_pos - top_padding_radio[top_joint_index] * unit_length).astype(int)\n",
    "        bottom = (bottom_pos + bottom_padding_radio[bottom_joint_index] * unit_length).astype(int)\n",
    "        bbox = (left, top, right, bottom)\n",
    "\n",
    "        cropped_img = self.crop_image(img, bbox)\n",
    "        return cropped_img, bbox\n",
    "\n",
    "\n",
    "    def crop_face(self, img, person_pose, unit_length):\n",
    "        face_size = unit_length\n",
    "        face_img = None\n",
    "        bbox = None\n",
    "\n",
    "        # if have nose\n",
    "        if person_pose[JointType.Nose][2] > 0:\n",
    "            nose_pos = person_pose[JointType.Nose][:2]\n",
    "            face_top = int(nose_pos[1] - face_size * 1.2)\n",
    "            face_bottom = int(nose_pos[1] + face_size * 0.8)\n",
    "            face_left = int(nose_pos[0] - face_size)\n",
    "            face_right = int(nose_pos[0] + face_size)\n",
    "            bbox = (face_left, face_top, face_right, face_bottom)\n",
    "            face_img = self.crop_image(img, bbox)\n",
    "\n",
    "        return face_img, bbox\n",
    "\n",
    "\n",
    "    def crop_hands(self, img, person_pose, unit_length):\n",
    "        hands = {\n",
    "            \"left\": None,\n",
    "            \"right\": None\n",
    "        }\n",
    "\n",
    "        if person_pose[JointType.LeftHand][2] > 0:\n",
    "            crop_center = person_pose[JointType.LeftHand][:-1]\n",
    "            if person_pose[JointType.LeftElbow][2] > 0:\n",
    "                direction_vec = person_pose[JointType.LeftHand][:-1] - person_pose[JointType.LeftElbow][:-1]\n",
    "                crop_center += (0.3 * direction_vec).astype(crop_center.dtype)\n",
    "            hand_img, bbox = self.crop_around_keypoint(img, crop_center, unit_length * 0.95)\n",
    "            hands[\"left\"] = {\n",
    "                \"img\": hand_img,\n",
    "                \"bbox\": bbox\n",
    "            }\n",
    "\n",
    "        if person_pose[JointType.RightHand][2] > 0:\n",
    "            crop_center = person_pose[JointType.RightHand][:-1]\n",
    "            if person_pose[JointType.RightElbow][2] > 0:\n",
    "                direction_vec = person_pose[JointType.RightHand][:-1] - person_pose[JointType.RightElbow][:-1]\n",
    "                crop_center += (0.3 * direction_vec).astype(crop_center.dtype)\n",
    "            hand_img, bbox = self.crop_around_keypoint(img, crop_center, unit_length * 0.95)\n",
    "            hands[\"right\"] = {\n",
    "                \"img\": hand_img,\n",
    "                \"bbox\": bbox\n",
    "            }\n",
    "\n",
    "        return hands\n",
    "\n",
    "    def crop_image(self, img, bbox):\n",
    "        left, top, right, bottom = bbox\n",
    "        img_h, img_w, img_ch = img.shape\n",
    "        box_h = bottom - top\n",
    "        box_w = right - left\n",
    "\n",
    "        crop_left = max(0, left)\n",
    "        crop_top = max(0, top)\n",
    "        crop_right = min(img_w, right)\n",
    "        crop_bottom = min(img_h, bottom)\n",
    "        crop_h = crop_bottom - crop_top\n",
    "        crop_w = crop_right - crop_left\n",
    "        cropped_img = img[crop_top:crop_bottom, crop_left:crop_right]\n",
    "\n",
    "        bias_x = bias_y = 0\n",
    "        if left < crop_left:\n",
    "            bias_x = crop_left - left\n",
    "        if top < crop_top:\n",
    "            bias_y = crop_top - top\n",
    "\n",
    "        # pad\n",
    "        padded_img = np.zeros((box_h, box_w, img_ch), dtype=np.uint8)\n",
    "        padded_img[bias_y:bias_y+crop_h, bias_x:bias_x+crop_w] = cropped_img\n",
    "\n",
    "        return padded_img\n",
    "\n",
    "    def __call__(self, orig_img, fast_mode=False):\n",
    "        orig_img_h, orig_img_w, _ = orig_img.shape\n",
    "\n",
    "        resized_output_img_w, resized_output_img_h = self.compute_optimal_size(orig_img, params['heatmap_size'])\n",
    "\n",
    "        pafs_sum = 0\n",
    "        heatmaps_sum = 0\n",
    "        # use only the first scale on fast mode\n",
    "        scales = [0.5] if fast_mode else params['inference_scales']\n",
    "\n",
    "        for scale in scales:\n",
    "            print(\"Inference scale: %.1f...\" % (scale))\n",
    "            img_size = int(params['inference_img_size'] * scale)\n",
    "            resized_input_img_w, resized_input_img_h = self.compute_optimal_size(orig_img, img_size)\n",
    "\n",
    "            resized_image = cv2.resize(orig_img, (resized_input_img_w, resized_input_img_h))\n",
    "            x_data = np.array(resized_image[np.newaxis], dtype=np.float32).transpose(0, 3, 1, 2) / 256 - 0.5\n",
    "\n",
    "            if self.device >= 0:\n",
    "                x_data = cuda.to_gpu(x_data)\n",
    "\n",
    "            h1s, h2s = self.model(x_data)\n",
    "\n",
    "            pafs_sum += F.resize_images(h1s[-1], (resized_output_img_h, resized_output_img_w)).data[0]\n",
    "            heatmaps_sum += F.resize_images(h2s[-1], (resized_output_img_h, resized_output_img_w)).data[0]\n",
    "\n",
    "        pafs = pafs_sum / len(scales)\n",
    "        heatmaps = heatmaps_sum / len(scales)\n",
    "        \n",
    "        self.pafs = pafs #for test\n",
    "        self.heatmaps = heatmaps #for test\n",
    "        \n",
    "        \n",
    "        if self.device >= 0:\n",
    "            pafs = pafs.get()\n",
    "\n",
    "        all_peaks = self.compute_peaks_from_heatmaps(heatmaps)\n",
    "        if len(all_peaks) == 0:\n",
    "            return np.empty((0, len(JointType), 3))\n",
    "        all_connections = self.compute_connections(pafs, all_peaks, resized_output_img_w, params)\n",
    "        subsets = self.grouping_key_points(all_connections, all_peaks, params)\n",
    "        all_peaks[:, 1] *= orig_img_w / resized_output_img_w\n",
    "        all_peaks[:, 2] *= orig_img_h / resized_output_img_h\n",
    "        person_pose_array = self.subsets_to_person_pose_array(subsets, all_peaks)\n",
    "        return person_pose_array\n",
    "\n",
    "\n",
    "def draw_person_pose(oriImg, person_pose):\n",
    "    if len(person_pose) == 0:\n",
    "        return oriImg\n",
    "\n",
    "    limb_colors = [\n",
    "        [0, 255, 0], [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255],\n",
    "        [0, 85, 255], [255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0.],\n",
    "        [255, 0, 85], [170, 255, 0], [85, 255, 0], [170, 0, 255.], [0, 0, 255],\n",
    "        [0, 0, 255], [255, 0, 255], [170, 0, 255], [255, 0, 170],\n",
    "    ]\n",
    "\n",
    "    joint_colors = [\n",
    "        [255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0],\n",
    "        [85, 255, 0], [0, 255, 0], [0, 255, 85], [0, 255, 170], [0, 255, 255],\n",
    "        [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255], [170, 0, 255],\n",
    "        [255, 0, 255], [255, 0, 170], [255, 0, 85]]\n",
    "\n",
    "    canvas = oriImg.copy()\n",
    "\n",
    "    # limbs\n",
    "    for pose in person_pose:\n",
    "        for i, (limb, color) in enumerate(zip(params['limbs_point'], limb_colors)):\n",
    "            if i != 9 and i != 13:  # don't show ear-shoulder connection\n",
    "                limb_ind = np.array(limb)\n",
    "                if np.all(pose[limb_ind][:, 2] != 0):\n",
    "                    joint1, joint2 = pose[limb_ind][:, :2]\n",
    "                    cv2.line(canvas, tuple(joint1), tuple(joint2), color, 2)\n",
    "\n",
    "    # joints\n",
    "    for pose in person_pose:\n",
    "        for i, ((x, y, v), color) in enumerate(zip(pose, joint_colors)):\n",
    "            if v != 0:\n",
    "                cv2.circle(canvas, (x, y), 6, color, -1)\n",
    "    return canvas\n",
    "\n",
    "\n",
    "def load_list(path):\n",
    "    tuples = []\n",
    "    for line in open(path):\n",
    "        pair = line.replace(\"\\n\",\"\")#.strip().split()\n",
    "        #print(pair)\n",
    "        tuples.append(pair)\n",
    "    return tuples\n",
    "\n",
    "#print(load_list(\"train.txt\"))\n",
    "\n",
    "def pos_feat_diff(feat,b_po,af_po):#Position feature difference\n",
    "    batch,ch,w,h = feat.data.shape\n",
    "    b_po = F.broadcast_to(b_po,(batch,ch,w,h)) #(1,1,46,46) to (1,128,46,46)\n",
    "    af_po = F.broadcast_to(af_po,(batch,ch,w,h))\n",
    "    feat = feat - b_po + af_po\n",
    "    return feat\n",
    "\n",
    "def pos_feat_sum(po):#Position Features Summary  shape (19,320,320)\n",
    "    po = F.sum(po,axis=0,keepdims=True)\n",
    "    po = F.resize_images(F.expand_dims(po,axis=0),(46,46))\n",
    "    return po #shape (1,1,46,46)\n",
    "\n",
    "class DisNet(chainer.Chain):\n",
    "    insize = 548\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DisNet, self).__init__(\n",
    "            # cnn to make feature map\n",
    "            conv1_1=L.Convolution2D(in_channels=3, out_channels=64, ksize=3, stride=1, pad=1),\n",
    "            conv1_2=L.Convolution2D(in_channels=64, out_channels=64, ksize=4, stride=2, pad=1),\n",
    "            conv2_1=L.Convolution2D(in_channels=64, out_channels=128, ksize=3, stride=1, pad=1),\n",
    "            conv2_2=L.Convolution2D(in_channels=128, out_channels=128, ksize=4, stride=2, pad=1),\n",
    "            conv3_1=L.Convolution2D(in_channels=128, out_channels=256, ksize=3, stride=1, pad=1),\n",
    "            conv3_2=L.Convolution2D(in_channels=256, out_channels=256, ksize=3, stride=1, pad=1),\n",
    "            conv3_3=L.Convolution2D(in_channels=256, out_channels=256, ksize=3, stride=1, pad=1),\n",
    "            conv3_4=L.Convolution2D(in_channels=256, out_channels=256, ksize=4, stride=2, pad=1),\n",
    "            conv4_1=L.Convolution2D(in_channels=256, out_channels=512, ksize=3, stride=1, pad=1),\n",
    "            conv4_2=L.Convolution2D(in_channels=512, out_channels=512, ksize=3, stride=1, pad=1),\n",
    "            conv4_3_CPM=L.Convolution2D(in_channels=512, out_channels=256, ksize=3, stride=1, pad=1),\n",
    "            conv4_4_CPM=L.Convolution2D(in_channels=256, out_channels=128, ksize=3, stride=1, pad=1),\n",
    "            l5 = L.Linear(None,1),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = F.leaky_relu(self.conv1_1(x))\n",
    "        h = F.leaky_relu(self.conv1_2(h))\n",
    "        #h = F.max_pooling_2d(h, ksize=2, stride=2)\n",
    "        h = F.leaky_relu(self.conv2_1(h))\n",
    "        h = F.leaky_relu(self.conv2_2(h))\n",
    "        #h = F.max_pooling_2d(h, ksize=2, stride=2)\n",
    "        h = F.leaky_relu(self.conv3_1(h))\n",
    "        h = F.leaky_relu(self.conv3_2(h))\n",
    "        h = F.leaky_relu(self.conv3_3(h))\n",
    "        h = F.leaky_relu(self.conv3_4(h))\n",
    "        #h = F.max_pooling_2d(h, ksize=2, stride=2)\n",
    "        h = F.leaky_relu(self.conv4_1(h))\n",
    "        h = F.leaky_relu(self.conv4_2(h))\n",
    "        h = F.leaky_relu(self.conv4_3_CPM(h))\n",
    "        h = F.leaky_relu(self.conv4_4_CPM(h))\n",
    "        h = self.l5(h)\n",
    "        return h\n",
    "    \n",
    "class GenNet(chainer.Chain):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GenNet, self).__init__(\n",
    "            # cnn to make feature map\n",
    "            conv1_1=L.Deconvolution2D(in_channels=64, out_channels=3, ksize=3, stride=1, pad=1),\n",
    "            conv1_2=L.Deconvolution2D(in_channels=64, out_channels=64, ksize=3, stride=1, pad=1),\n",
    "            conv2_1=L.Deconvolution2D(in_channels=128, out_channels=64, ksize=3, stride=1, pad=1),\n",
    "            conv2_2=L.Deconvolution2D(in_channels=128, out_channels=128, ksize=3, stride=1, pad=1),\n",
    "            conv3_1=L.Deconvolution2D(in_channels=256, out_channels=128, ksize=3, stride=1, pad=1),\n",
    "            conv3_2=L.Deconvolution2D(in_channels=256, out_channels=256, ksize=3, stride=1, pad=1),\n",
    "            conv3_3=L.Deconvolution2D(in_channels=256, out_channels=256, ksize=3, stride=1, pad=1),\n",
    "            conv3_4=L.Deconvolution2D(in_channels=256, out_channels=256, ksize=3, stride=1, pad=1),\n",
    "            conv4_1=L.Deconvolution2D(in_channels=512, out_channels=256, ksize=3, stride=1, pad=1),\n",
    "            conv4_2=L.Deconvolution2D(in_channels=512, out_channels=512, ksize=3, stride=1, pad=1),\n",
    "            conv4_3_CPM=L.Deconvolution2D(in_channels=256, out_channels=512, ksize=3, stride=1, pad=1),\n",
    "            conv4_4_CPM=L.Deconvolution2D(in_channels=128, out_channels=256, ksize=3, stride=1, pad=1),\n",
    "            upsamp14 = L.Deconvolution2D(in_channels=256, out_channels=256, ksize=4, stride=2, pad=1),\n",
    "            upsamp12 = L.Deconvolution2D(in_channels=128, out_channels=128, ksize=4, stride=2, pad=1),\n",
    "            upsamp11 = L.Deconvolution2D(in_channels=64, out_channels=64, ksize=4, stride=2, pad=1),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = F.leaky_relu(self.conv4_4_CPM(x))\n",
    "        h = F.leaky_relu(self.conv4_3_CPM(h))\n",
    "        h = F.leaky_relu(self.conv4_2(h))\n",
    "        h = F.leaky_relu(self.conv4_1(h))\n",
    "        h = F.leaky_relu(self.upsamp14(h))\n",
    "        h = F.leaky_relu(self.conv3_4(h))\n",
    "        h = F.leaky_relu(self.conv3_3(h))\n",
    "        h = F.leaky_relu(self.conv3_2(h))\n",
    "        h = F.leaky_relu(self.conv3_1(h))\n",
    "        h = F.leaky_relu(self.upsamp12(h))\n",
    "        h = F.leaky_relu(self.conv2_2(h))\n",
    "        h = F.leaky_relu(self.conv2_1(h))\n",
    "        h = F.leaky_relu(self.upsamp11(h))\n",
    "        h = F.leaky_relu(self.conv1_2(h))\n",
    "        h = F.leaky_relu(self.conv1_1(h))\n",
    "        return h\n",
    "\n",
    "\n",
    "def load_list(path):\n",
    "    tuples = []\n",
    "    for line in open(path):\n",
    "        pair = line.replace(\"\\n\",\"\")#.strip().split()\n",
    "        #print(pair)\n",
    "        tuples.append(pair)\n",
    "    return tuples\n",
    "\n",
    "#print(load_list(\"train.txt\"))\n",
    "\n",
    "def pos_feat_diff(feat,b_po,af_po):#Position feature difference\n",
    "    batch,ch,w,h = feat.data.shape\n",
    "    b_po = F.broadcast_to(b_po,(batch,ch,w,h)) #(1,1,46,46) to (1,128,46,46)\n",
    "    af_po = F.broadcast_to(af_po,(batch,ch,w,h))\n",
    "    feat = feat - b_po + af_po\n",
    "    return feat\n",
    "\n",
    "def pos_feat_sum(po):#Position Features Summary  shape (19,320,320)\n",
    "    po = F.sum(po,axis=0,keepdims=True)\n",
    "    po = F.resize_images(F.expand_dims(po,axis=0),(46,46))\n",
    "    return po #shape (1,1,46,46)\n",
    "\n",
    "class MEVIN(chainer.Chain):\n",
    "    insize = 320\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MEVIN, self).__init__(\n",
    "            mlp1_1 = chainer.ChainList(\n",
    "                *[L.Linear(None,42*42)\n",
    "                  for i in range(19)]),\n",
    "            mlp1_2 = chainer.ChainList(\n",
    "                *[L.Linear(None,4)\n",
    "                  for i in range(19)]),\n",
    "            mlp2_1 = chainer.ChainList(\n",
    "                *[L.Linear(None,4)\n",
    "                  for i in range(19*19)]),\n",
    "            mlp2_2 = chainer.ChainList(\n",
    "                *[L.Linear(None,2)\n",
    "                  for i in range(19*19)]),\n",
    "            mlp3_1 = L.Linear(None,46*46),\n",
    "            mlp3_2 = L.Linear(None,46*46),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x1,x2,x3,x4):#Pred = [x1,x2,x3,x4]\n",
    "        #print(x1.shape)\n",
    "        h1 = self.Mix_pofe(x1,x2,x3)\n",
    "        h2 = self.Mix_pofe(x2,x3,x4)\n",
    "        \n",
    "        #print(len(h1))\n",
    "        h1 = self.Interaction(h1)\n",
    "        h2 = self.Interaction(h2)\n",
    "        \n",
    "        h = self.Aggregator(h1,h2) # (1,1,42,42)\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    \n",
    "    def Mix_pofe(self,b_x,af_x,aff_x):        \n",
    "        b_x_list = F.split_axis(b_x, 19, axis=0) #[(1,320,320),......]\n",
    "        af_x_list = F.split_axis(af_x, 19, axis=0) #[(1,320,320),......]\n",
    "        aff_x_list = F.split_axis(aff_x, 19, axis=0)\n",
    "        \n",
    "        afb_x = []\n",
    "        for i in range(len(b_x_list)):\n",
    "            aff_af = F.concat((af_x_list[i],aff_x_list[i]),axis=1)\n",
    "            afb_x.append(F.expand_dims(F.concat((b_x_list[i],aff_af),axis=1),axis=0)) #(1,3,320,320)\n",
    "            \n",
    "        return afb_x\n",
    "    \n",
    "    def Interaction(self,afb_x): \n",
    "        F_mlp = []\n",
    "        for i in range(len(afb_x)):\n",
    "            img = F.resize_images(afb_x[i],(46,46))\n",
    "            pred = F.leaky_relu(self.mlp1_1[i](img))#(1,3,42,42) >> (1,42*42)\n",
    "            pred = F.leaky_relu(self.mlp1_2[i](pred))#(1,24*24) >> (1,4)\n",
    "            F_mlp.append(pred)\n",
    "        \n",
    "        self_inter = []\n",
    "        inter = 0\n",
    "        for i in range(len(F_mlp)):\n",
    "            for ii in range(len(F_mlp)):\n",
    "                #print(i)\n",
    "                #print(ii)\n",
    "                if i == ii:\n",
    "                    h = F.concat((F_mlp[i],F_mlp[ii]),axis=1)\n",
    "                    h = F.leaky_relu(self.mlp2_1[i * 19 + ii](h)) #(1,4)\n",
    "                    #print(\"chack\",h)\n",
    "                    self_inter.append(self.mlp2_2[i * 19 + ii](h)) #(1,2)\n",
    "                elif i != ii:\n",
    "                    h = F.concat((F_mlp[i],F_mlp[ii]),axis=1)\n",
    "                    h = F.leaky_relu(self.mlp2_1[i * 19 + ii](h)) #(1,4)\n",
    "                    #print(\"test\",h)\n",
    "                    inter += F.leaky_relu(self.mlp2_2[i * 19 + ii](h))#(1,2)\n",
    "                                          \n",
    "        for iii in range(len(self_inter)):\n",
    "            self_inter[iii] += inter\n",
    "                                          \n",
    "        self_inter_add_inter = F.stack(self_inter, axis=1)\n",
    "        \n",
    "        return self_inter_add_inter #(1,2*19)\n",
    "                    \n",
    "    def Aggregator(self,si_add1,si_add2):\n",
    "        h = F.concat((si_add1,si_add2),axis=1)#(1,2*19*2)\n",
    "        h = F.leaky_relu(self.mlp3_1(h)) #(1,42*42)\n",
    "        h = F.leaky_relu(self.mlp3_2(h)) #(1,42*42)\n",
    "        h = F.reshape(h,(h.data.shape[0],1,46,46))\n",
    "        return h\n",
    "    \n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2)学習済みデータのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PoseNet...\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "pose_detector = PoseDetector('posenet', \"models/coco_posenet.npz\", device=-1)\n",
    "mevin = MEVIN()\n",
    "serializers.load_npz(\"mevin/model_300.npz\", mevin)\n",
    "gen = GenNet()\n",
    "serializers.load_npz(\"gen/gen.npz\", gen)\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3)スタートフレーム位置、予測フレーム数設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#スタート地点　0～40フレーム\n",
    "start_frame = 7\n",
    "\n",
    "#予測フレーム数\n",
    "num_frame = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4)実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_42133.jpg\n",
      "Inference scale: 1.0...\n",
      "1_42134.jpg\n",
      "Inference scale: 1.0...\n",
      "1_42135.jpg\n",
      "Inference scale: 1.0...\n",
      "1_42136.jpg\n",
      "Inference scale: 1.0...\n",
      "Inference scale: 1.0...\n",
      "Inference scale: 1.0...\n",
      "Inference scale: 1.0...\n",
      "Inference scale: 1.0...\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "pad = np.ones((19,320,320),dtype=np.float32)\n",
    "pad_zero = np.zeros((320,320),dtype=np.float32)\n",
    "irekae = [9,10,12,13,16,17,18]\n",
    "for ii in irekae:\n",
    "    pad[ii] = pad_zero\n",
    "\n",
    "test_list = load_list(\"train.txt\")\n",
    "\n",
    "Preds = []\n",
    "Feture = []\n",
    "\n",
    "for i in range(4):#test_list:\n",
    "    ii = i+start_frame\n",
    "    img = cv2.imread(\"use_data/\"+test_list[ii])\n",
    "    print(test_list[ii])\n",
    "    person_pose_array = pose_detector(img)\n",
    "    pose_detector.heatmaps = pose_detector.heatmaps * pad\n",
    "    Preds.append(pose_detector.heatmaps)\n",
    "    \n",
    "    if i == 3:\n",
    "        Feture.append(pose_detector.model.feature_map)\n",
    "\n",
    "for i in range(num_frame):\n",
    "    if i == 0:\n",
    "        po1 = pos_feat_sum(Preds[3])\n",
    "        \n",
    "    with chainer.using_config('train', 'False'):\n",
    "        po2 = mevin(Preds[0],Preds[1],Preds[2],Preds[3])\n",
    "        \n",
    "        feat = pos_feat_diff(Feture[0],po1,po2)\n",
    "        \n",
    "        out_img = gen(feat).data\n",
    "        \n",
    "    cv2.imwrite('fe_result/fe_img/f_img'+str(i)+'.png', (out_img.transpose(0,2,3,1)[0]+0.5)*256)\n",
    "    cv2.imwrite('fe_result/po_img/po_img'+str(i)+'.png', (F.resize_images(po2,(42*5,42*5)).data.transpose(0,2,3,1)[0])*256)\n",
    "    \n",
    "    po1 = po2\n",
    "    img = (out_img.transpose(0,2,3,1)[0]+0.5)*256\n",
    "    person_pose_array = pose_detector(img)\n",
    "    \n",
    "    img_print = draw_person_pose(img, person_pose_array)\n",
    "    cv2.imwrite('fe_result/pe_img/pe_img'+ str(i) + '.png', img_print)\n",
    "    \n",
    "    pose_detector.heatmaps = pose_detector.heatmaps * pad\n",
    "    Preds.pop(0)\n",
    "    Preds.append(pose_detector.heatmaps)\n",
    "    Feture.pop(0)\n",
    "    Feture.append(pose_detector.model.feature_map)\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
